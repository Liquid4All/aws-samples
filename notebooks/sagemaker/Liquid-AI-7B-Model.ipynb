{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Liquid AI 7B Model Package from AWS Marketplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers the Liquid 7B model targeting L40S GPU. \n",
    "\n",
    "Learn more on https://www.liquid.ai/lfm-7b .\n",
    "\n",
    "\n",
    "The model runs on all of the G6E range instance type family https://aws.amazon.com/ec2/instance-types/g6e/\n",
    "including the smallest `ml.g6e.xlarge` instance and the multi GPU instances, though using a single GPU is cost-effective.\n",
    "\n",
    "| instance type   | GPU | vCPU | RAM(GB)|\n",
    "|-----------------|-----|------|--------|\n",
    "| ml.g6e.xlarge   |   1 |    4 |     32 |\n",
    "| ml.g6e.2xlarge  |   1 |    8 |     64 |\n",
    "| ml.g6e.4xlarge  |   1 |   16 |    128 |\n",
    "| ml.g6e.8xlarge  |   1 |   32 |    256 |\n",
    "| ml.g6e.16xlarge |   1 |   64 |    512 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to the model package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open the model package listing [page](https://aws.amazon.com/marketplace/pp/prodview-a7hfjtjhloy36).\n",
    "2. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "3. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agrees with EULA, pricing, and support terms. \n",
    "4. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_arn = \"<Customer to specify Model package ARN corresponding to their AWS region>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Initiate a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate a session\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Specify the instance type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to understand how real-time inference with Amazon SageMaker works, see [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smallest 1GPU instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.g6e.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More powerful 1GPU instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.g6e.16xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a deployable model from the model package.\n",
    "model = sagemaker.ModelPackage(\n",
    "    role=role, model_package_arn=model_package_arn, sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy the model\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1, instance_type=instance_type, endpoint_name=model.name\n",
    ")\n",
    "# endpoint name is available after the deployment\n",
    "endpoint_name = model.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint has been created, you would be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model encourages to use a chat template, for more details see https://platform.openai.com/docs/api-reference/chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "body = {\n",
    "    # has to be always supplied\n",
    "    \"model\": \"/opt/ml/model\",\n",
    "    # your conversation\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is deep learning in 2 sentences?\"\n",
    "        }\n",
    "    ],\n",
    "    # extras\n",
    "    \"max_tokens\": 512,\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "response = sagemaker_session.sagemaker_runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    Body=json.dumps(body),\n",
    ")\n",
    "\n",
    "json.load(response[\"Body\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Streaming example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "body = {\n",
    "    # differnet from the previous example\n",
    "    \"stream\": True,\n",
    "    # has to be always supplied\n",
    "    \"model\": \"/opt/ml/model\",\n",
    "    # your conversation\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is deep learning in 2 sentences?\"\n",
    "        }\n",
    "    ],\n",
    "    # extras\n",
    "    \"max_tokens\": 512,\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "response = sagemaker_session.sagemaker_runtime_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    Body=json.dumps(body),\n",
    ")\n",
    "\n",
    "def walk():\n",
    "    last_chunk = \"\"\n",
    "    for chunk in response[\"Body\"]:\n",
    "        proc_chunk = chunk[\"PayloadPart\"][\"Bytes\"].decode('utf_8').replace(\"data:\", \"\").replace(\"[DONE]\", \"\").strip()\n",
    "        try:\n",
    "            payload = last_chunk + proc_chunk\n",
    "            response_json = json.loads(payload)\n",
    "            print(response_json)\n",
    "            last_chunk = \"\"\n",
    "            yield response_json['choices'][0]['delta']['content']\n",
    "        except (json.decoder.JSONDecodeError):\n",
    "            # partial response, re-try on the next iteration\n",
    "            last_chunk = proc_chunk\n",
    "\n",
    "total_response = ''.join(walk())\n",
    "total_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. You can terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(endpoint_name)\n",
    "model.sagemaker_session.delete_endpoint_config(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Unsubscribe to the listing (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to unsubscribe to the model package, follow these steps. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package or using the algorithm. Note - You can find this information by looking at the container name associated with the model. \n",
    "\n",
    "**Steps to unsubscribe to product from AWS Marketplace**:\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust)\n",
    "2. Locate the listing that you want to cancel the subscription for, and then choose __Cancel Subscription__  to cancel the subscription.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
